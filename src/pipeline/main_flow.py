import sys
import os

# Th√™m ƒëo·∫°n n√†y l√™n ƒë·∫ßu file folder_watcher.py
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, '..', '..'))
if project_root not in sys.path:
    sys.path.append(project_root)

import time
from watchdog.observers import Observer



import pandas as pd
import yaml
import os
import shutil
import time
from src.utils.logger import setup_logger
from src.validation.validator import DataValidator
from src.processing.transform import DataTransformer
from src.storage.db_handler import DBHandler
from src.serving.notifier import Notifier

# Load Config
CONFIG_PATH = "config/settings.yaml"
if not os.path.exists(CONFIG_PATH):
    raise FileNotFoundError(f"Config file not found at {CONFIG_PATH}")

with open(CONFIG_PATH, "r") as f:
    config = yaml.safe_load(f)

# Kh·ªüi t·∫°o c√°c module
logger = setup_logger()
db = DBHandler(config['database']['connection_string'])
validator = DataValidator(config['schemas'])
transformer = DataTransformer()
notifier = Notifier() # Email user/pass c√≥ th·ªÉ l·∫•y t·ª´ bi·∫øn m√¥i tr∆∞·ªùng

def run_pipeline(file_path):
    filename = os.path.basename(file_path)
    logger.info(f"üöÄ --- B·∫ÆT ƒê·∫¶U X·ª¨ L√ù: {filename} ---")
    
    start_time = time.time()
    total_rows = 0
    clean_rows = 0
    error_rows = 0
    
    try:
        # --- B∆Ø·ªöC 1: DATA INGESTION ---
        # ƒê·ªçc file v√†o DataFrame
        if filename.endswith('.csv'):
            df = pd.read_csv(file_path)
        elif filename.endswith(('.xls', '.xlsx')):
            df = pd.read_excel(file_path)
        else:
            msg = f"ƒê·ªãnh d·∫°ng file kh√¥ng h·ªó tr·ª£: {filename}"
            logger.error(msg)
            db.log_process(filename, 0, 0, 0, "SKIPPED", msg)
            return

        total_rows = len(df)
        
        # Backup file g·ªëc v√†o Raw Zone
        raw_path = os.path.join(config['data_dirs']['raw'], filename)
        shutil.copy(file_path, raw_path)
        logger.info(f"ƒê√£ l∆∞u backup t·∫°i: {raw_path}")

        # --- B∆Ø·ªöC 2: DATA VALIDATION ---
        clean_df, error_df, val_msg = validator.validate(df, filename)
        
        # N·∫øu l·ªói nghi√™m tr·ªçng (vd: thi·∫øu c·ªôt b·∫Øt bu·ªôc) -> D·ª´ng pipeline
        if clean_df is None:
            logger.error(f"‚ùå Validation Critical Fail: {val_msg}")
            db.log_process(filename, total_rows, 0, total_rows, "FAILED", val_msg)
            
            # G·ª≠i email c·∫£nh b√°o Admin
            notifier.send_email(
                "admin@smb-hub.com", 
                f"L·ªñI NGHI√äM TR·ªåNG: {filename}", 
                f"File b·ªã t·ª´ ch·ªëi x·ª≠ l√Ω.\nL√Ω do: {val_msg}"
            )
            return

        # X·ª≠ l√Ω c√°c d√≤ng l·ªói (n·∫øu c√≥)
        error_rows = len(error_df)
        if not error_df.empty:
            error_path = os.path.join(config['data_dirs']['error'], f"error_{filename}")
            error_df.to_csv(error_path, index=False)
            logger.warning(f"‚ö†Ô∏è Ph√°t hi·ªán {error_rows} d√≤ng l·ªói. Chi ti·∫øt t·∫°i: {error_path}")

        # --- B∆Ø·ªöC 3: TRANSFORMATION ---
        processed_df = transformer.process(clean_df)
        clean_rows = len(processed_df)

        # --- B∆Ø·ªöC 4: STORAGE & SERVING ---
        if not processed_df.empty:
            # [LOGIC M·ªöI] T·ª± ƒë·ªông ch·ªçn t√™n b·∫£ng d·ª±a tr√™n t√™n file
            fname_lower = filename.lower()
            if "sv" in fname_lower or "student" in fname_lower or "ds" in fname_lower:
                target_table = "students_list"  # L∆∞u v√†o b·∫£ng ri√™ng cho sinh vi√™n
            else:
                target_table = "customers_telco" # M·∫∑c ƒë·ªãnh l∆∞u v√†o b·∫£ng Telco
            
            # L∆∞u v√†o Database v·ªõi t√™n b·∫£ng ƒë·ªông
            db.save_clean_data(processed_df, table_name=target_table)
            
            # L∆∞u file s·∫°ch ra folder
            clean_path = os.path.join(config['data_dirs']['clean'], f"clean_{filename}")
            processed_df.to_csv(clean_path, index=False)
            
            # ƒê·ªìng b·ªô Google Sheet (Gi·∫£ l·∫≠p)
            notifier.sync_to_google_sheet(processed_df, "Data_Report")

        # --- B∆Ø·ªöC 5: LOGGING METADATA ---
        status = "SUCCESS" if error_rows == 0 else "WARNING"
        processing_time = round(time.time() - start_time, 2)
        
        db.log_process(filename, total_rows, clean_rows, error_rows, status, f"Time: {processing_time}s")
        
        success_msg = (
            f"‚úÖ Ho√†n th√†nh x·ª≠ l√Ω file {filename}.\n"
            f"- T·ªïng: {total_rows}\n"
            f"- S·∫°ch: {clean_rows}\n"
            f"- L·ªói: {error_rows}"
        )
        logger.info(success_msg)

        # G·ª≠i email b√°o c√°o n·∫øu c·∫ßn (ho·∫∑c ch·ªâ g·ª≠i khi c√≥ warning)
        if status == "WARNING":
            notifier.send_email("manager@smb-hub.com", f"B√°o c√°o x·ª≠ l√Ω: {filename}", success_msg)

        # Cleanup: X√≥a file trong uploads sau khi xong ƒë·ªÉ tr√°nh x·ª≠ l√Ω l·∫°i
        if os.path.exists(file_path):
            os.remove(file_path)

    except Exception as e:
        logger.error(f"‚ùå L·ªñI H·ªÜ TH·ªêNG (CRASH): {str(e)}")
        db.log_process(filename, total_rows, 0, 0, "CRASH", str(e))
        notifier.send_email("admin@smb-hub.com", "SYSTEM CRASH", str(e))

if __name__ == "__main__":
    # Ch·∫ø ƒë·ªô ch·∫°y th·ªß c√¥ng: Qu√©t to√†n b·ªô folder uploads
    upload_dir = config['data_dirs']['upload']
    logger.info(f"ƒêang qu√©t th∆∞ m·ª•c: {upload_dir}")
    if not os.path.exists(upload_dir):
        os.makedirs(upload_dir)
        
    files = [f for f in os.listdir(upload_dir) if os.path.isfile(os.path.join(upload_dir, f))]
    
    if not files:
        logger.info("Kh√¥ng c√≥ file n√†o trong th∆∞ m·ª•c uploads.")
    else:
        for f in files:
            run_pipeline(os.path.join(upload_dir, f))